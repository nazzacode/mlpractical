\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{icml2017}
\citation{simonyan2014very,he2016deep}
\citation{glorot2010understanding}
\citation{bengio1993problem}
\citation{he2016deep}
\citation{glorot2010understanding}
\citation{glorot2010understanding}
\citation{bishop1995neural}
\citation{ioffe2015batch}
\citation{he2016deep,huang2017densely}
\citation{ioffe2015batch}
\citation{he2016deep}
\citation{rumelhart1986learning}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:loss_curves}{{1a}{1}{Loss per epoch\relax }{figure.caption.1}{}}
\newlabel{sub@fig:loss_curves}{{a}{1}{Loss per epoch\relax }{figure.caption.1}{}}
\newlabel{fig:acc_curves}{{1b}{1}{Accuracy per epoch\relax }{figure.caption.1}{}}
\newlabel{sub@fig:acc_curves}{{b}{1}{Accuracy per epoch\relax }{figure.caption.1}{}}
\newlabel{fig:curves}{{1}{1}{Training curves for VGG08 and VGG38\relax }{figure.caption.1}{}}
\newlabel{sec:task1}{{2}{1}{}{section.2}{}}
\citation{bengio1993problem}
\citation{simonyan2014very}
\citation{ioffe2015batch}
\citation{lecun2012efficient}
\citation{santurkar2018does}
\citation{he2016deep}
\newlabel{fig:grad_flow_08}{{2}{2}{Gradient flow on VGG08\relax }{figure.caption.2}{}}
\newlabel{fig:grad_flow_38}{{3}{2}{Gradient Flow on VGG38\relax }{figure.caption.3}{}}
\newlabel{eq.fprop}{{1}{2}{}{equation.2.1}{}}
\newlabel{eq.bprop}{{2}{2}{}{equation.2.2}{}}
\newlabel{sec:lit_rev}{{3}{2}{}{section.3}{}}
\citation{ioffe2015batch}
\citation{he2016deep}
\citation{krizhevsky2009learning}
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\newlabel{fig:training_curves_bestModel}{{4}{4}{Training curves for ? ? ?\relax }{figure.caption.4}{}}
\newlabel{fig:grad_flow_bestModel}{{5}{4}{Gradient Flow on ? ? ?\relax }{figure.caption.5}{}}
\newlabel{sec:disc}{{6}{4}{}{section.6}{}}
\citation{kolesnikov2020bigTransfer}
\bibdata{refs}
\bibcite{bengio1993problem}{{1}{1993}{{Bengio et~al.}}{{Bengio, Frasconi, and Simard}}}
\bibcite{bishop1995neural}{{2}{1995}{{Bishop et~al.}}{{}}}
\bibcite{glorot2010understanding}{{3}{2010}{{Glorot \& Bengio}}{{Glorot and Bengio}}}
\bibcite{he2016deep}{{4}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{huang2017densely}{{5}{2017}{{Huang et~al.}}{{Huang, Liu, Van Der~Maaten, and Weinberger}}}
\bibcite{ioffe2015batch}{{6}{2015}{{Ioffe \& Szegedy}}{{Ioffe and Szegedy}}}
\bibcite{kolesnikov2020bigTransfer}{{7}{2020}{{Kolesnikov et~al.}}{{Kolesnikov, Beyer, Zhai, Puigcerver, Yung, Gelly, and Houlsby}}}
\bibcite{krizhevsky2009learning}{{8}{2009}{{Krizhevsky et~al.}}{{Krizhevsky, Hinton, et~al.}}}
\bibcite{lecun2012efficient}{{9}{2012}{{LeCun et~al.}}{{LeCun, Bottou, Orr, and M{\"u}ller}}}
\bibcite{rumelhart1986learning}{{10}{1986}{{Rumelhart et~al.}}{{Rumelhart, Hinton, and Williams}}}
\bibcite{santurkar2018does}{{11}{2018}{{Santurkar et~al.}}{{Santurkar, Tsipras, Ilyas, and M{\k {a}}dry}}}
\bibcite{simonyan2014very}{{12}{2014}{{Simonyan \& Zisserman}}{{Simonyan and Zisserman}}}
\newlabel{tab:CIFAR_results}{{1}{5}{Experiment results (number of model parameters, Training and Validation loss and accuracy) for different combinations of VGG08, VGG38, Batch Normalisation (BN), and Residual Connections (RC), LR is learning rate.\relax }{table.caption.6}{}}
\newlabel{sec:concl}{{7}{5}{}{section.7}{}}
\gdef \@abspage@last{5}
